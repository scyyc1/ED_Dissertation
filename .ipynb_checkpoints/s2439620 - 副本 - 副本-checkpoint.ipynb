{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b7e173",
   "metadata": {},
   "source": [
    "**<p style='text-align: right;'>Ver. 1.0.3</p>**\n",
    "\n",
    "# Introductory Applied Machine Learning (IAML) Coursework - Semester 2, 2022-23\n",
    "\n",
    "### Author: Hiroshi Shimodaira and Rohan Gorantla\n",
    "\n",
    "## Important Instructions\n",
    "\n",
    "#### It is important that you follow the instructions below carefully for things to work properly.\n",
    "\n",
    "You need to set up and activate your environment as you would do for your labs, see Learn section on Labs.  **You will need to use Noteable to create the files you will submit (the Jupyter (IPynthon) Notebook and the PDF)**.  Do **NOT** create the PDF in some other way, we will not be able to mark it.  If you want to develop your answers in your own environment, you should make sure you are using the same packages we are using, by running the cell which does imports below.\n",
    "\n",
    "Read the instructions in this notebook carefully, especially where asked to name variables with a specific name. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers. In most cases we indicate the nature of answer we are expecting (code/text), and also provide the required code/markdown cell.\n",
    "\n",
    "- We will use the IAML Learn page for any announcements, updates, and FAQs on this assignment. Please ***visit the page frequently*** to find the latest information/changes.\n",
    "- Data files that you will be using are included in the coursework zip file that you have downloaded from the Learn assignment page for this coursework.\n",
    "- There is a helper file 'iaml23cw_helpers.py' in the zip file, which you should upload to your environment.\n",
    "- Some of the topics in this coursework are covered in weeks 7 and 8 of the course. Focus first on questions on topics that you have covered already, and come back to the other questions as the lectures progress.\n",
    "- Keep your answers brief and concise.\n",
    "- Make sure to show all your code/working.\n",
    "- All the figures you present should have axis labels, titles, and grid lines unless specified explicitly. If you think grid lines spoiling readability, you can adjust the line width and/or line style. Figures should not be too small to read.\n",
    "- Write readable code. While we do not expect you to follow PEP8 to the letter, the code should be adequately understandable, with plots/visualisations correctly labelled. Do use inline comments when doing something non-standard.\n",
    "- When asked to present numerical values, make sure to represent real numbers in the appropriate precision corresponding to your answer. \n",
    "- When you use libraries specified in this coursework, you should use the default parameters unless specified explicitly.\n",
    "- The criteria on which you will be judged include the quality of the textual answers and/or any plots asked for. For higher marks, when asked you need to give good and concise discussions based on experiments and theories using your own words.\n",
    "\n",
    "- You will see <html>\\\\pagebreak</html> at the start of each subquestion.  ***Do not remove these, if you do we will not be able to mark your coursework.***\n",
    "\n",
    "#### Good Scholarly Practice\n",
    "Please remember the University requirement regarding all assessed work for credit. Details about this can be found at:\n",
    "http://web.inf.ed.ac.uk/infweb/admin/policies/academic-misconduct\n",
    "\n",
    "Specifically, this assignment should be your own individual work. We will employ tools for detecting misconduct.\n",
    "\n",
    "Moreover, please note that Piazza is NOT a forum for discussing the solutions of the assignment. You may ask private questions. You can use the office hours to ask questions.\n",
    "\n",
    "### SUBMISSION Mechanics\n",
    "This assignment will account for 30% of your final mark. We ask you to submit answers to all questions.\n",
    "\n",
    "You will submit (1) a PDF of your Notebook and (2) the Notebook itself via Gradescope.  Your grade will be based on the PDF, we will only use the Notebook if we need to see details.  **You must use the following procedure to create the materials to submit**.\n",
    "\n",
    "1. Make sure your Notebook, the helper file, and the datasets are in Noteable and will run.  If you developed your answers in Noteable, this is already done.\n",
    "\n",
    "2. Select **Kernel->Restart & Run All** to create a clean copy of your submission, this will run the cells in order from top to bottom.  This may take a while (a few hours) to complete, ensure that all the output and plots have complete before you proceed.\n",
    "\n",
    "3. Select **File->Download as->PDF via LaTeX (.pdf)** and wait for the PDF to be created and downloaded.\n",
    "\n",
    "4. Select **File->Download as->Notebook (.ipynb)**\n",
    "\n",
    "5. You now should have in your download folder the pdf and the notebook.  Rename them sNNNNNNN.pdf and sNNNNNNN.ipynb, where sNNNNNNN is your matriculation number (student number).\n",
    "\n",
    "**Details on submission instructions will be announced and documented on Learn before the deadline**. \n",
    "\n",
    "The submission deadline for this assignment is **28th March 2023 at 12:00 (midday) UK time (UTC)**.  Don't leave it to the last minute!\n",
    "\n",
    "\n",
    "#### IMPORTS\n",
    "Execute the cell below to import all packages you will be using for this assignment.  If you are not using Noteable, make sure the python and package version numbers reported match the python and package numbers, which can be checked by running the following cell. The Python version does not need to be the same, but it should be $3.9.p$, where $p \\ge 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11f0c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\t3.11.0 <=> 3.9.12\n",
      "Scipy\t1.9.3 <=> 1.7.3\n",
      "Numpy\t1.23.5 <=> 1.21.6\n",
      "Sklearn\t1.1.3 <=> 1.1.1\n",
      "Pandas\t1.5.2 <=> 1.4.2\n",
      "Matplotlib\t3.6.2 <=> 3.5.2\n",
      "Seaborn\t0.12.2 <=> 0.11.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from iaml23cw_helpers import *\n",
    "print_versions();\n",
    "\n",
    "# You may add other libraries here or in your other cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3503f",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4508de8",
   "metadata": {},
   "source": [
    "# Question 1: Experiments with a stock price  data set\n",
    "\n",
    "#### 65 marks out of 130 for this coursework\n",
    "\n",
    "The stock price data set we use in this coursework is a stock market index (composite stock price index of common stocks) in a country for the period between 2000 and 2022, consisting of four historical prices ('Open', 'High', 'Low', 'Close', which denote the opening, highest, lowest, and closing prices on the trading day, respectively) and trading volume. For the convenience of the coursework, we have added some features to the data set. They are four [technical indicators](https://www.fidelity.com/bin-public/060_www_fidelity_com/documents/learning-center/Understanding-Indicators-TA.pdf) (RSI, SMA, BBP, ADX), 'Tomorrow', and 'Target'. 'Tomorrow' holds the closing price of next trading day, which we will use for price prediction, and 'Target' is a binary indicator (label), which takes 1 if 'Tomorrow' is higher than 'Close', 0 otherwise, which we will use for the prediction of movement direction.\n",
    "\n",
    "*** Loading data ***\n",
    "Make sure that you have the data set files \"dset_q1a.csv\" and \"dset_q1b.csv\" in your environment. We will use the first file in the following sub questions except the last subquestion 1.8. Run the following cell to load the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17c90f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dset_q1a.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data set \"dset_q1a.csv\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdset_q1a.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Env\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dset_q1a.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data set \"dset_q1a.csv\"\n",
    "df = pd.read_csv(\"dset_q1a.csv\", index_col=\"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2943069",
   "metadata": {},
   "source": [
    "# ========== Question 1.1 --- [5 marks] ==========\n",
    "###  Describe the main properties of the data:\n",
    "1. [Code] Display the shape of the data\n",
    "2. [Code] Display the range of the dataframe index\n",
    "3. [Code] What data are present and what types of data are they? Display the information using **pandas.DataFrame.info**.\n",
    "4. [Code] Display the highest price, the lowest price, and the mean of the closing price ('Close') for each year in the data. (Hint: the highest price for each year is sought from the price 'High'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8301fd5",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "print(\"The shape of the data: {}\".format(df.shape))\n",
    "# print('Number of samples: {}, number of attributes: {}'.format(df.shape[0], df.drop('Target', axis=1).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "print(\"The date range from {} to {}\".format(df.index.min(), df.index.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a7731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#(4) Your code goes here\n",
    "yearly_stats = df.groupby(df.index.year).agg({'High': 'max', 'Low': 'min', 'Close': 'mean'})\n",
    "print(yearly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46c2ad",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7b325",
   "metadata": {},
   "source": [
    "# ========== Question 1.2 --- [8 marks] ==========\n",
    "Perform an *exploratory data analysis* on the dataset by studying the following:\n",
    "1. [Code and text] Plot the stock market closing price ('Close') and comment on it.\n",
    "2. [Code] For the period from the beginning of year 2007 until the end of 2008, plot the closing price ('Close') and volumes ('Volume') respectively, where you show months on the x-axis and indicate the positions of the highest and lowest values for the period.\n",
    "3. [Code and text] Plot a pairplot for the dataset features using the seaborn **pairplot** and report the patterns in the given dataset.\n",
    "4. [Code] Plot the correlation matrix for the dataset features.\n",
    "5. [Text] Based on the results you obtained in 3 and 4 above, comment on the relationships among the features present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd28e2",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code and text goes here\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Stock Market Closing Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66f4d5",
   "metadata": {},
   "source": [
    "**Commets:**\n",
    "\n",
    "Generally, the closing price of the stock market increased year by year. However, few oscillations could be spotted in 2008-2009 and 2020-2021. This might be due to the influence of global financial crisis and the Covid. But after the sharp decrease in 2020, the closing price also presented a sharp rise tendency. This might indicate the inflation of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94216ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code and text goes here\n",
    "# Close price:\n",
    "df_filtered = df.loc['2007-01-01':'2008-12-31']\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_filtered['Close'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Stock Market Closing Price (2007-2008)')\n",
    "\n",
    "# The highest and lowest price\n",
    "max_idx = df_filtered['Close'].idxmax()\n",
    "min_idx = df_filtered['Close'].idxmin()\n",
    "plt.scatter([max_idx], [df_filtered.loc[max_idx, 'Close']], color='red', label='Highest value')\n",
    "plt.scatter([min_idx], [df_filtered.loc[min_idx, 'Close']], color='green', label='Lowest value')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code and text goes here\n",
    "# Close price:\n",
    "df_filtered = df.loc['2007-01-01':'2008-12-31']\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df_filtered['Volume'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Stock Market Closing Price (2007-2008)')\n",
    "\n",
    "# The highest and lowest price\n",
    "max_idx = df_filtered['Volume'].idxmax()\n",
    "min_idx = df_filtered['Volume'].idxmin()\n",
    "plt.scatter([max_idx], [df_filtered.loc[max_idx, 'Volume']], color='red', label='Highest value')\n",
    "plt.scatter([min_idx], [df_filtered.loc[min_idx, 'Volume']], color='green', label='Lowest value')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10571c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code and text goes here\n",
    "g = sns.pairplot(data=df, vars=df.columns, hue='Target', height=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330572b9",
   "metadata": {},
   "source": [
    "The 'Open', 'High', 'Low', and 'Close' prices show a strong positive linear correlation, indicating that the prices tend to move together. 'Volume' seems to have little to no correlation with the other features. The technical indicators (RSI, SMA, BBP, ADX) show some patterns with each other and with the price features, but they are not as clear-cut as the relationships among the price features. Some of these relationships can be non-linear or more complex. The 'Target' feature, which is binary, does not show any clear pattern with the other features in the pairplot. This is expected, as it is a derived feature that indicates whether the 'Tomorrow' price is higher than the 'Close' price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) Your code goes here\n",
    "correlation_matrix = df.corr()\n",
    "# print(correlation_matrix)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, vmin=0., vmax=1., annot=True)\n",
    "plt.title('Correlation Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca341a",
   "metadata": {},
   "source": [
    "#(5) Your text goes here\n",
    "\n",
    "Overall, we can see that all of these features are independent to 'Target'.\n",
    "\n",
    "However, the 'Open', 'High', 'Low', 'Close', 'SMA' and 'Tomorrow' features have strong possitive correlation (correlation $\\approx$ 1), which means that they are not mutually independent features (redundant feature). This is expected since these prices represent different aspects of the stock prices during the trading day. Besides, the 'RSI' and 'BBP' also shows a highly positive correlation (correlation = 0.9), which means that these two features may \"works\" in similar way to the task. In the pair plot showed above, all these pair of redundant features are presented in linear curve.\n",
    "\n",
    "In naive Bayes classifier, we assume the value of a particular feature is independent of the value of any other features. So these data might need additional processing (drops and leave one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23cd97",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1303cb",
   "metadata": {},
   "source": [
    "# ========== Question 1.3 --- [9 marks] ==========\n",
    "\n",
    "We here apply linear regression to predict 'Tomorrow' from 'SMA'.\n",
    "For this question, you should use the sklearn implementation of Linear Regression. Use the first 80% of the data for training and the rest 20% for testing ***without shuffling***.\n",
    "1. [Code] Fit a linear regression model to the training data so that we can predict 'Tomorrow' from 'SMA'. Report the estimated model parameters w and the coefficient of determination $R^2$.\n",
    "2. [Text] Describe what the parameters represent for the fitted dataset with the linear regression model.\n",
    "3. [Code] Report the root mean-square error (RMSE) for the training set and test set, respectively.\n",
    "4. [Code] Plot predicted values versus actual values for the test set, where the x-axis corresponds to actual values and the y-axis to predicted values. Draw a line of $y=x$ on the plot.\n",
    "5. [Code] Plot 'Tomorrow' versus 'SMA' for the training set and display the regression line on the same graph. The x-axis corresponds to 'SMA' and the y-axis to 'Tomorrow'.\n",
    "6. [Text] Examining the results (e.g. $R^2$ and RMSE), comment on the predictability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a43b4f",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "X = df['SMA'].values.reshape(-1,1)\n",
    "y = df['Tomorrow'].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "LR = LinearRegression(fit_intercept=True, normalize=True, copy_X=True)\n",
    "LR.fit(X_train,y_train)\n",
    "# print(LR.coef_)\n",
    "\n",
    "# Not provided in the original version of assessment script\n",
    "# But this function is used in the lab, so I use here\n",
    "from sklearn.metrics import r2_score\n",
    "# Or calculate in the following formula, should give the same answer\n",
    "def R2_score(y_pred, y_real):\n",
    "    y_mean = np.mean(y_real)\n",
    "    total = np.sum((y_real - y_mean) ** 2)\n",
    "    residual = np.sum((y_real - y_pred) ** 2)\n",
    "    return 1 - (residual / total)\n",
    "\n",
    "y_pred_test = LR.predict(X_test)\n",
    "\n",
    "print('Estimated model parameters w is: {:.3f}'.format(LR.coef_[0][0]))\n",
    "print('Coefficient of determination R2 is: {:.3f}'.format(r2_score(y_test, y_pred_test)))\n",
    "# print('Coefficient of determination R2 is: {:.3f}'.format(R2_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abdadee",
   "metadata": {},
   "source": [
    "#(2) Your text goes here\n",
    "\n",
    "Since we only use the 'SMA' feature to fit the linear regression model, in this case, two parameters are involved: **Weight $w$** & **bias $b$**. The model is represented in the following form: $$prediction = wX + b$$where $X$, $w$ and $b$ are single numerical numbers.\n",
    "\n",
    "The **weight $w$** represents the relationship between the independent variable 'SMA' and the dependent variable 'Tomorrow'. In this context, the weight shows how much the 'Tomorrow' price changes for a unit change in the 'SMA'. If the weight is positive, it means that an increase in 'SMA' is associated with an increase in the 'Tomorrow' price. The magnitude of the weight indicates the strength of this relationship.\n",
    "\n",
    "The **bias $b$** represents the baseline value of the dependent variable 'Tomorrow' when the independent variable 'SMA' is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00188f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here\n",
    "\n",
    "# Also not provided\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Or use the following function, should give the same answer\n",
    "def RMSE(y_pred, y_real):\n",
    "    return np.sqrt(np.mean((y_real - y_pred) ** 2))\n",
    "\n",
    "y_pred_train = LR.predict(X_train)\n",
    "# y_pred_train_temp= LR.coef_[0][0] * X_train + LR.intercept_\n",
    "# print((y_pred_train == y_pred_train_temp).all())\n",
    "\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "## Or\n",
    "# RMSE_train = RMSE(y_train, y_pred_train)\n",
    "# RMSE_test = RMSE(y_test, y_pred_test)\n",
    "\n",
    "print('RSME of train dataset: {:.3f}'.format(RMSE_train))\n",
    "print('RSME of test dataset: {:.3f}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58656607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) Your code goes here\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test,y_pred_test)\n",
    "min_val = min(min(y_test), min(y_pred))\n",
    "max_val = max(max(y_test), max(y_pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='black')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d78bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(5) Your code goes here\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, label='Training Data')\n",
    "plt.xlabel('SMA')\n",
    "plt.ylabel('Tomorrow')\n",
    "plt.title('Linear Regression Line for the Training Set')\n",
    "plt.plot(X_train, y_pred_train, color='red', label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345cfa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The bias b of the model is: {:.3f}'.format(LR.intercept_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c2c91",
   "metadata": {},
   "source": [
    "#(6) Your text goes here\n",
    "\n",
    "Generally, as it is illustrated in the question 1.2, the 'SMA' and 'Tomorrow' features are highly linear correlated.\n",
    "\n",
    "From the perspective of the coefficient of determination $R^2$ (ranges from 0 to 1), the value for this model is 0.988, which is closed to 1, which means the feature 'Tomorrow' can be effectively predicted from the feature 'SMA' with linear model.\n",
    "\n",
    "In turns of RMSE, it is used quantify the difference between the predicted and actual values. The RMSE for test set in this case is 70.708, which is also very small with regard to the scale of the dataset. This also indicates that the linear model fit well with provided data. All of these two metric indicate that 'SMA' and 'Tomorrow' features are highly linear correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1993ba49",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed87a29",
   "metadata": {},
   "source": [
    "# ========== Question 1.4 --- [5 marks] ==========\n",
    "\n",
    "1. [Code] Instead of using libraries for linear regression, write the code of your own for finding the regression coefficients of the regression model that predicts 'Tomorrow' from 'SMA'. Run your code and show the coefficients, where you should use the same training data as Question 1.3.\n",
    "2. [Text] One of the common metric used for evaluating the performance of regression models is Mean Squared Error (MSE). Write out the expression for MSE and list one of its limitations and how it can be addressed with alternative metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10dad6",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f203657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(5) Your code goes here\n",
    "x_mean = sum(X_train) / len(X_train)\n",
    "y_mean = sum(y_train) / len(y_train)\n",
    "\n",
    "# The covariance of X and Y\n",
    "covariance = sum((x - x_mean) * (y - y_mean) for x, y in zip(X_train, y_train))\n",
    "# The variance of X\n",
    "variance = sum((value - x_mean)**2 for value in X_train)\n",
    "\n",
    "\n",
    "# prediction = w * x + b\n",
    "w = covariance / variance\n",
    "b = y_mean - w * x_mean\n",
    "\n",
    "print('Regression coefficient is: {:.3f}'.format(w[0]))\n",
    "print('Is the result equal to the question 1.3?', round(w[0], 3) == round(LR.coef_[0][0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4e5a0",
   "metadata": {},
   "source": [
    "#(2) Your text goes here\n",
    "\n",
    "The formula of MSE is: $$\\text{MSE} = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "The limitation of MSE is that it is sensitive to outliers. This is because the squared differences are magnified for larger deviations from the predicted values. Alternative metrics Mean Absolute Error (MAE) is less sensitive to outliers. The expression for MAE is: $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}^{(i)} - y^{(i)}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5274d5f",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5560a",
   "metadata": {},
   "source": [
    "# ========== Question 1.5 --- [6 marks] ==========\n",
    "#### Multiple linear regression and polynomial regression\n",
    "\n",
    "We here consider multiple linear regression that employs four variables ('RSI', 'SMA', 'BBP', 'ADX') to predict 'Tomorrow'. We use the same training data and test data as Question 1.3.\n",
    "1. [Code] Train the multiple linear regression model on the training set and show the model parameters and the coefficient of determination $R^2$. You also show the RMSE for the training set and test set respectively.\n",
    "2. [Code] We now extend the model to the polynomial regression model, in which we use all polynomial combinations of the variables up to the specified degree $p$. Using $p=2$, run an experiment in the same manner as 1 above and report the model parameters and $R^2$. You also report the RMSE for the training and test sets respectively. You should use the sklearn implementation of Linear Regression and Polynomial Features. \n",
    "3. [Text] Comparing the results you obtained here and those in Question 1.3, report your findings and give discussions briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3175e",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2261882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "feature_names = ['RSI','SMA','BBP','ADX']\n",
    "X_multiple = df[feature_names].values\n",
    "y_multiple = df['Tomorrow'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multiple, y_multiple, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "LR_multiple = LinearRegression()\n",
    "LR_multiple.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train = LR_multiple.predict(X_train)\n",
    "y_pred_test = LR_multiple.predict(X_test)\n",
    "\n",
    "for index, feature in enumerate(feature_names):\n",
    "    print('The estimated model parameters (weight) for {} is {:.3f}'.format(feature, LR_multiple.coef_[index]))\n",
    "print(\"The coefficients (in array form): {}\".format(LR_multiple.coef_))\n",
    "print('Intercept: ',LR_multiple.intercept_)\n",
    "print('The coefficient of determination R2: {:.3f}'.format(r2_score(y_test, y_pred_test)))\n",
    "\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print('RSME of training set: {:.3f}'.format(RMSE_train))\n",
    "print('RSME of testing set: {:.3f}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "#(2) Your code goes here\n",
    "\n",
    "# Generate polynomial and interaction features.\n",
    "PF_transformer = PolynomialFeatures(degree=2)\n",
    "\n",
    "X_train_poly = PF_transformer.fit_transform(X_train)\n",
    "X_test_poly = PF_transformer.fit_transform(X_test)\n",
    "\n",
    "PR = LinearRegression()\n",
    "PR.fit(X_train_poly,y_train)\n",
    "\n",
    "y_pred_train_poly = PR.predict(X_train_poly)\n",
    "y_pred_test_poly = PR.predict(X_test_poly)\n",
    "\n",
    "print(\"The coefficients: \\n{}\".format(PR.coef_))\n",
    "print('Intercept: ',PR.intercept_)\n",
    "print('The coefficient of determination R2: {:.3f}'.format(r2_score(y_test, y_pred_test_poly)))\n",
    "\n",
    "RMSE_train = np.sqrt(mean_squared_error(y_train, y_pred_train_poly))\n",
    "RMSE_test = np.sqrt(mean_squared_error(y_test, y_pred_test_poly))\n",
    "print('RSME of training set: {:.3f}'.format(RMSE_train))\n",
    "print('RSME of testing set: {:.3f}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9150e",
   "metadata": {},
   "source": [
    "#(3) Your text goes here\n",
    "\n",
    "Overall, compared to the result of question 1.3, the polynomial regression model has similar outstanding performance on the same dataset. In turns of coefficient of determination $R^2$, the value is 0.988, which is also close to 1. This shows that the feature 'Tomorrow' can be effectively predicted from the feature 'SMA' with linear model.\n",
    "\n",
    "In turns of RMSE, the polynomial regression model proform sligtly better (with lower magnitude). This means the prediction values on test set are much closer to the real values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3ffe4",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44b755",
   "metadata": {},
   "source": [
    "# ========== Question 1.6 --- [12 marks] ==========\n",
    "#### Classification\n",
    "\n",
    "We now consider the prediction of stock price movement as a binary classification problem - class 1 for upward movement and class 0 otherwise. We use the four technical Indicators, 'RSI', 'SMA', 'BBP', 'ADX', as input features to a classifier to predict 'Target'.\n",
    "\n",
    "1. [Code] Using 10-fold cross validation with ***no shuffling*** on ***the whole data***, train four classifiers, Logistic Regression, SVM, Decision Trees, and Random Forests. Display, in a single graph, the validation accuracy with boxplot for each model. For each model, you also report the mean accuracy and mean F-score for the training set and validation set, respectively.\n",
    "(NB: You should obtain the accuracy and F-score for each trial of k-fold cross validation, which will be used for plotting a boxplot. A mean value/score denotes the average value over the $k$ trials, where $k=10$).\n",
    "<br> ***Note***: you should use sklearn's KFold, SVC, DecisionTreeClassifier, RandomForestClassifier, and LogisticRegression. For each classification model, use default parameters except that \"***random_state=0***\" should be specified.\n",
    "2. [Code] Further to the above, for each model, display the confusion matrix for the validation sets, where rows correspond to true class labels and columns to predicted ones, and each element of the matrix shows the number of corresponding instances.\n",
    "3. [Text] Comment on which model is best with respect to false positives and false negatives. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7baf33",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "# Not included in the original script\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "X_clf = df[feature_names].values\n",
    "y_clf = df['Target'].values\n",
    "KF = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "baselines = ['prior', 'most_frequent']\n",
    "models = [\n",
    "    (\"Dommy Classifier\", DummyClassifier(strategy='most_frequent')),\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=0)),\n",
    "    (\"SVM\", SVC(random_state=0)),\n",
    "    (\"Decision Trees\", DecisionTreeClassifier(random_state=0)),\n",
    "    (\"Random Forests\", RandomForestClassifier(random_state=0))\n",
    "]\n",
    "\n",
    "clf_results = []\n",
    "# Training processes\n",
    "for name, model in models:\n",
    "    ACC_train = []\n",
    "    ACC_test = []\n",
    "    F_train = [] \n",
    "    F_test = []\n",
    "    cm = np.zeros((2, 2))\n",
    "    for train_index, test_index in KF.split(X_clf):\n",
    "        X_train, X_test = X_clf[train_index], X_clf[test_index]\n",
    "        y_train, y_test = y_clf[train_index], y_clf[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Accuracy and F-score of each batch (totolly 10 folds)\n",
    "        ACC_train.append(accuracy_score(y_train, y_pred_train))\n",
    "        ACC_test.append(accuracy_score(y_test, y_pred_test))\n",
    "        F_train.append(f1_score(y_train, y_pred_train))\n",
    "        F_test.append(f1_score(y_test, y_pred_test))\n",
    "        \n",
    "        # Confusion matrix for the next question\n",
    "        cm += confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    clf_results.append((name, ACC_train, ACC_test, F_train, F_test, cm))\n",
    "    \n",
    "    print('Mean Training Accuracy for {}: {:.3f}'.format(name, np.mean(ACC_train)))\n",
    "    print('Mean Test Accuracy for {}: {:.3f}'.format(name, np.mean(ACC_test)))\n",
    "    print('Mean Training F1-score for {}: {:.3f}'.format(name, np.mean(F_train)))\n",
    "    print('Mean Test F1-score for {}: {:.3f}\\n'.format(name, np.mean(F_test)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax.boxplot([ACC_test for _, _, ACC_test, _, _, _ in clf_results])\n",
    "ax.set_xticklabels([name for name, _, _, _, _, _ in clf_results])\n",
    "ax.set_title('Accuracy with 10-fold Cross-Validation')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "# From lab material\n",
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(5,5))\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "classes = ['Lower than yesterday', 'Higher than yesterday']\n",
    "for index, data in enumerate(clf_results):\n",
    "    print('Confusion Matrix of {} is:'.format(data[0]))\n",
    "    print(data[5], '\\n')\n",
    "    cm = data[5]\n",
    "#     cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=classes, title='Confusion matrix of Validation for {}'.format(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839b579",
   "metadata": {},
   "source": [
    "#(3) Your text goes here\n",
    "\n",
    "Generally, all of the four models classifiers yield to a not very satisfactory result.\n",
    "\n",
    "For SVM and Logistic Regression, both models predict a great duel of false instances (positive and negative). This means that during the training process, these two models tend to make optimistic prediction (The closing price of 'Tomorrow' is greater than today). I also work out the confusion matrix of dummy classifier for contrast purpose. From the graph we can learn that both of the model have similar performace as dummy classifier, who predict the dominant class with probability of 1, with the accuracy for test set are around 0.53 and the f-score around 0.69.\n",
    "\n",
    "For the Decision Tree and Random Forest, according to the confusion matrices, both models have almost equal amount of predictions on both true and false instances. Because I did not set the 'max_depth ' property for decision tree, they will always perfectly separate the training set (overfitting on training set and lead to identical confusion matrix). The accuracy on test set is also around 0.5, however, the Random Forest, which based on random subspace method (slightly mitigate the influence of outliners), perform slightly better than single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad2ba5",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc12ea8",
   "metadata": {},
   "source": [
    "# ========== Question 1.7 --- [5 marks] ==========\n",
    "#### Dimensionality Reduction \n",
    "Here we will perform dimensionality reduction with PCA to the data and run classification experiments on the dimensionality reduced data.\n",
    "\n",
    "1. [Code] Using the four technical features ('RSI', 'SMA', 'BBP', 'ADX') as input data, apply PCA to ***the whole data*** and find the minimum set of principal components that explains at least 95% of the variance of the data. Report the number of principal components in the set you found.\n",
    "2. [Code] Using the set of principal components you found above, reduce the dimensionality of the data and run classification experiments for the four classifiers in the same manner as we did in Question 1.6, but we now use the dimensionality-reduced data instead. You should plot boxplots and report accuracy and F-score in the same manner as Question 1.6. (Note that this experiment is not a formal one, as we apply PCA to the whole data, whose subset is used for testing.)\n",
    "3. [Text] Comparing the results with those you obtained in Question 1.6, report your findings and give brief discussions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd2d62",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3db486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "X = df[feature_names].values\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X)\n",
    "y_pca = df['Target'].values\n",
    "\n",
    "print('Number of principal components: {}'.format(pca.n_components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ad1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "pca_results = []\n",
    "# Training processes\n",
    "for name, model in models:\n",
    "    ACC_train = []\n",
    "    ACC_test = []\n",
    "    F_train = [] \n",
    "    F_test = []\n",
    "    cm = np.zeros((2, 2))\n",
    "    for train_index, test_index in KF.split(X_pca):\n",
    "        X_train, X_test = X_pca[train_index], X_pca[test_index]\n",
    "        y_train, y_test = y_pca[train_index], y_pca[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Accuracy and F-score of each batch (totolly 10 folds)\n",
    "        ACC_train.append(accuracy_score(y_train, y_pred_train))\n",
    "        ACC_test.append(accuracy_score(y_test, y_pred_test))\n",
    "        F_train.append(f1_score(y_train, y_pred_train))\n",
    "        F_test.append(f1_score(y_test, y_pred_test))\n",
    "        \n",
    "        # Confusion matrix for the next question\n",
    "        cm += confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    pca_results.append((name, ACC_train, ACC_test, F_train, F_test, cm))\n",
    "    \n",
    "    print('Mean Training Accuracy for {}: {:.3f}'.format(name, np.mean(ACC_train)))\n",
    "    print('Mean Test Accuracy for {}: {:.3f}'.format(name, np.mean(ACC_test)))\n",
    "    print('Mean Training F1-score for {}: {:.3f}'.format(name, np.mean(F_train)))\n",
    "    print('Mean Test F1-score for {}: {:.3f}\\n'.format(name, np.mean(F_test)))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax.boxplot([ACC_test for _, _, ACC_test, _, _, _ in pca_results])\n",
    "ax.set_xticklabels([name for name, _, _, _, _, _ in pca_results])\n",
    "ax.set_title('Accuracy with 10-fold Cross-Validation')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54d814",
   "metadata": {},
   "source": [
    "#(3) Your text goes here\n",
    "\n",
    "By comparing both accuracy and f-score to the result of question 1.6, we can find that the application of PCA have limited influence on the overall result. All of the statistics even drop down slightly. This is because that the objective of PCA is to preserve as much details while reducing the demention of original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe5392",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aed46d",
   "metadata": {},
   "source": [
    "# ========== Question 1.8 --- [15 marks] ==========\n",
    "\n",
    "\n",
    "We considered only four technical features so far to find that movement classification with the four classifiers is challenging.\n",
    "We would like to know whether we could improve the performance if we use more features, apply preprocessing to the data, and tune up model parameters.\n",
    "To find some answer to the question, carry out a mini project with the following conditions:\n",
    "* We use another data set file (\"dset_q1b.csv\") for this project, which is an extended version of the original one and contains 15 technical indicators. Load the dataset in the following manner:\n",
    ">   df1b = pd.read_csv(\"dset_q1b.csv\", index_col=\"Date\", parse_dates=True)\n",
    "* We consider SVM (SVC) only.\n",
    "* We split the data into two subsets without shuffling - the first 80% of data should be used for training and validation, and the remaining 20% for testing. \n",
    "* We will limit the duration of the project to a few hours only.\n",
    "* The outcome of the project is not necessarily positive. It is not surprising that you cannot find much improvement.\n",
    "\n",
    "1. [Text] Describe your ideas for improving the classification performance. Your ideas should be concrete and feasible - the project should be done in the specified time length.\n",
    "2. [Code and text] Implement your ideas, run experiments, and report the results including accuracy and F-score for the training set and test set respectively.\n",
    "3. [Code and text] Examine whether your improvement or deterioration is statistically significant.\n",
    "4. [Text] Summarise your findings and show your answer to the question. In case of negative results, explain the reasons for the negative outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadadba",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc1e83",
   "metadata": {},
   "source": [
    "#(1) Your text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584786ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code and text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code and text goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428daa39",
   "metadata": {},
   "source": [
    "#(4) Your text goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22303f35",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b684e",
   "metadata": {},
   "source": [
    "# Question 2: Experiments with image data\n",
    "\n",
    "#### 65 marks out of 130 for this coursework\n",
    "\n",
    "Image data are made up of $H  W  C$ pixels, where $H, W, C$ denote the height, width, and the number of channels, respectively. For simplicity, we assume a grayscale image (i.e. $C=1$). Let $p_{ij}$ denote the pixel value at a grid point $(i,j), 1 \\le i \\le H, 1 \\le j \\le W$, where $p_{11}$ corresponds to the the pixel at the top-left corner and $p_{HW}$ to the one at the bottom-right corner. We assume that $p_{ij}$ takes an integer value between 0 and 255 (i.e. 8-bit coding). In computers, we can store a grayscale image of $\\{p_{ij}\\}$ in a $D$-dimensional vector, $x = (x_1,x_2,...,x_D)$, where $D = H \\times W$, and $x_1$ corresponds to $p_{11}$ and $x_D$ to $p_{HW}$.\n",
    "\n",
    "In this question, we use a subset of the [Fashion MNIST Dataset](https://github.com/zalandoresearch/fashion-mnist), which contains images of fashion products from ten categories (e.g. T-shirt and trousers). The ten categories are represented as integer numbers ($0,\\ldots,9$) and they are referred to as classes. There are 1000 training instances and 200 test instances per class. Each instance is a 28-by-28 grayscale image. Note that you will find some errors (e.g. incorrect labels) in the data set, but we use the data set as it is.\n",
    "Load the data and apply some pre-processing in the following manner in your code.\n",
    "\n",
    "***Loading data:***\n",
    "Make sure that you have the data set file \"dset_q2.mat\" in your environment and run the following cell to load the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2aad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set and apply some preprocessing\n",
    "\n",
    "Xtrn_org, Ytrn_org, Xtst_org, Ytst_org = load_q2_dataset()\n",
    "\n",
    "Xtrn = np.copy(Xtrn_org) / 255.0   # Training data : (10000, 784)\n",
    "Xtst = np.copy(Xtst_org) / 255.0   # Testing data : (2000, 784)\n",
    "Ytrn = np.copy(Ytrn_org)           # Labels for Xtrn : (10000,)\n",
    "Ytst = np.copy(Ytst_org)           # Labels for Xtst : (2000,)\n",
    "Xmean = np.mean(Xtrn, axis=0)\n",
    "Xtrn_mn = Xtrn - Xmean; Xtst_mn = Xtst - Xmean  # Mean-normalised versions of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b431fc",
   "metadata": {},
   "source": [
    "You can display the image of the fourth instance in **Xtrn** in the following manner, for example. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491601d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(1.0,1.0)) # You could try a much large fig size\n",
    "plt.imshow(Xtrn[4,:].reshape(28,28), cmap=plt.cm.gray_r);\n",
    "# plt.grid(lw=1, ls=':')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51e03d",
   "metadata": {},
   "source": [
    "# ========== Question 2.1 --- [5 marks] ==========\n",
    "[Code] For each class, display the grayscale images of the first five instances of the class in the training set **(Xtrn,Ytrn)**, where you should follow the specifications shown below.\n",
    "- You will display a total of 50 images, which should be displayed in a 10-by-5 grid,  where a grid point $(i,j), i=0,\\ldots,9, j=0,\\ldots,4$,  displays the image of $j$-th instance of class $i$.  Note that we use zero-based numbering here.\n",
    "- Use plt.imshow to display an image.\n",
    "- Specify the figure size by plt.figure(figsize=(10, 20)).\n",
    "- The image of each instance should be displayed properly in the right orientation.\n",
    "- For each image, you should display the class number and the instance number in **Xtrn**, for which you could use **pyplot.title**. For example, if the first instance of class 0 is held in **Xtrn[21,:]**, the instance number is 21, so that \"C0: 21\" (or \"0: 21\") may be the information you should display."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6bf5c",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234500a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# The classes and the counter array to count 5 picture\n",
    "classes = np.unique(Ytrn)\n",
    "count = np.zeros(classes.shape)\n",
    "\n",
    "# List of selected images\n",
    "selected_img = [[] for _ in range(len(classes))]\n",
    "index_instance = [[] for _ in range(len(classes))]\n",
    "\n",
    "# Get images\n",
    "for index, img in enumerate(Xtrn):\n",
    "    if(count[Ytrn[index]] < 5):\n",
    "        selected_img[Ytrn[index]].append(img)\n",
    "        index_instance[Ytrn[index]].append(index)\n",
    "        count[Ytrn[index]] += 1\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(10, 20))\n",
    "for j, batch in enumerate(selected_img):\n",
    "    for i, img in enumerate(batch):\n",
    "        plt.subplot(10, 5, j * 5 + i + 1)\n",
    "        plt.imshow(img.reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "        plt.title(\"C{}: {}\".format(j, index_instance[j][i]))\n",
    "        plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377edd6c",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43937688",
   "metadata": {},
   "source": [
    "# ========== Question 2.2 --- [11 marks] ==========\n",
    "\n",
    "You may have understood there is a wide variety of images in each class. We now would like to display the images of representative instances for each class in the training data set **(Xtrn, Ytrn)**. To that end, we apply the k-means clustering with $k = 6$ to each class. Instead of displaying the image of each cluster centre, which would look blurred due to averaging, we display the image of the instance that is closest to the centroid (i.e. cluster centre) as the representative of the cluster. We also display the mean image (i.e. the image of the mean vector) of each class.\n",
    "\n",
    "[Code] Following the specifications shown below, display the result.\n",
    "- For clustering, use sklearn's **KMeans** with the default parameters except that you specify **n_clusters=6** and **random_state=0**. Note that the two parameters should be specified explicitly when you run clustering for each class.\n",
    "- You will display a total of 60+10=70 images, which should be displayed in a 10-by-7 grid. Each row corresponds to a class. The grid point $(i, 0)$ displays the mean image of class $i$ data, and the grid point $(i, j), j=1,\\ldots,6$  displays the image of the representative of cluster $j$-1 for class $i$. Clusters should be sorted in increasing order in terms of the Euclidean distance to the centre of the class (i.e. the mean of the instances in the class), so that the column $j$=1 corresponds to the cluster that is closest to the class centre, whereas column $j$=6 to the one that is farthest from the class centre. Note that we use zero-based numbering.\n",
    "- For each image of an instance, display the class number ($c$), the number of instances ($m$) in the cluster, and the instance number ($\\ell$) in the training data set, in the format of \"C{$c$} [{$m$}] {$\\ell$}\". For example, \"C2 [165] 9734\" represents $c$=2. $m$=165, and $\\ell$=9734.\n",
    "- Use a large figure size for plotting, e.g. plt.figure(figsize=(16,20))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fea86",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d474080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classified imgaes with regards to classes\n",
    "img_classified = np.array([Xtrn[Ytrn == cls] for cls in classes])\n",
    "# The indices of these images\n",
    "img_classified_idx = [[] for _ in range(len(classes))]\n",
    "for idx, _ in enumerate(Xtrn):\n",
    "    img_classified_idx[Ytrn[idx]].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean images\n",
    "img_classified_mean = np.array([np.mean(batch, axis=0) for batch in img_classified])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64d7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNN for each classes of images\n",
    "img_clustered_models = []\n",
    "# To store the value of 'l'\n",
    "img_clustered_closest = []\n",
    "for cls, batch_of_imgs in enumerate(img_classified):\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    kmeans.fit(batch_of_imgs)\n",
    "    img_clustered_models.append(kmeans)\n",
    "#     print(kmeans.labels_)\n",
    "\n",
    "    # For each cluster\n",
    "    img_clustered_closest_idx = []\n",
    "    for cluster_idx in range(6):\n",
    "        img_clustered = np.array([img for img in batch_of_imgs[kmeans.labels_ == cluster_idx]])\n",
    "        img_clustered_index = np.array([img_classified_idx[cls][idx] for idx,_ in enumerate(batch_of_imgs) if kmeans.labels_[idx] == cluster_idx])\n",
    "        img_distances = np.array([np.linalg.norm(img-kmeans.cluster_centers_[cluster_idx]) for img in img_clustered])\n",
    "        img_clustered_closest_idx.append(img_clustered_index[np.argmin(img_distances)])\n",
    "        \n",
    "#         print(img_clustered_index[np.argmin(img_distances)])\n",
    "    \n",
    "    img_clustered_closest.append(img_clustered_closest_idx)\n",
    "# print(img_clustered_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The quantity of each cluster in each class, size is (10, 6) \n",
    "# To store the value of 'm'\n",
    "img_classified_clustered_count = [\n",
    "    [sum(kmeans_model.labels_ == cluster) for cluster in range(6)] for kmeans_model in img_clustered_models\n",
    "]\n",
    "# print(img_classified_clustered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daeb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 20))\n",
    "for idx, batch in enumerate(img_classified):\n",
    "    kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "    kmeans.fit(batch)\n",
    "    \n",
    "    # Plot the mean image of current class\n",
    "    first_img_idx = idx * 7 + 1\n",
    "    plt.subplot(10, 7, first_img_idx)\n",
    "    plt.imshow(img_classified_mean[idx].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "    plt.title(\"C{} mean\".format(idx))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot the rest\n",
    "    for j in range(6):\n",
    "        plt.subplot(10, 7, first_img_idx + j + 1)\n",
    "        m = img_classified_clustered_count[idx][j]\n",
    "        l = img_clustered_closest[idx][j]\n",
    "        \n",
    "        plt.imshow(Xtrn[l,:].reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "        plt.title(\"C{} [{}] {}\".format(idx, m, l))\n",
    "        plt.axis('off')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a112e0",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007403e9",
   "metadata": {},
   "source": [
    "# ========== Question 2.3 --- [7 marks] ==========\n",
    "1. [Code] Apply Principal Component Analysis (PCA) to the data of **Xtrn_mn** using sklearn's **PCA** and show the variances of projected data for the first five principal components. \n",
    "2. [Code] Plot a graph of the cumulative explained variance ratio $r_i$ as a function of the number of principal components, $i$, where $ 1 \\le i  D$, $r_i$ is defined as follows, and $D$ is the number of dimensions of the data.<br>\n",
    "> $$ r_i = \\frac{\\sum_{j=1}^i \\lambda_j}{\\sum_{j=1}^D \\lambda_j}$$\n",
    "3. [Code] Find the minimum number of principal components required to explain 50%, 60%, 70%, 80%, 90%, and 95% of the total variance, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bb13e",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78270c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "pca = PCA(n_components = Xtrn_mn.shape[1])\n",
    "pca.fit(Xtrn_mn)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Variance of Component {}: {:.5f}\".format(i+1, pca.explained_variance_ratio_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e1199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "EVR = pca.explained_variance_ratio_\n",
    "cumulative_EVR = [sum(explained_variance_ratios[:i+1]) for i in range(Xtrn_mn.shape[1])]\n",
    "    \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, Xtrn_mn.shape[1] + 1), cumulative_EVR)\n",
    "plt.xlabel('Number of Principal Components (i)')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio (ri)')\n",
    "plt.title('Cumulative explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) Your code goes here\n",
    "for ratio in [0.5,0.6,0.7,0.8,0.9,0.95]:\n",
    "    num = np.argmax(np.array(cumulative_EVR) >= ratio) + 1\n",
    "    print(\"The minimum value of principal components required to explain {}% of the total variance: {}\".format(ratio*100,num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c7f0a",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9cc53",
   "metadata": {},
   "source": [
    "# ========== Question 2.4 --- [10 marks] ==========\n",
    "We now consider a simple application of PCA, in which we (as sender A) apply dimensionality reduction to image samples and send them to someone (as receiver B) who tries to reconstruct the samples from the dimensionality-reduced samples. The underlying assumption is that the both parties, A and B, share the same set of principal components (i.e. eigen vectors) and the mean vector (**Xmean**) in advance.\n",
    "You will expect some degradation in the reconstructed images.\n",
    "1. [Code] Follow the instructions shown below.\n",
    "- Apply PCA to the whole **Xtrn_mn** at first to find all principal components. \n",
    "- For each class and for each number of principal components $K = 5,20,50,200,400$, apply the dimensionality reduction to the first instance in the class.\n",
    "- Display the reconstructed images and the original images in a 10-by-6 grid, where each row corresponds to a class (in increasing order) and the first five columns show the reconstructed images for the five values of $K$ (in increasing order) and the last column to shows the original image.\n",
    "- Note that you should add **Xmean** to each reconstructed data to display the corresponding image.\n",
    "2. [Text] Explain your findings briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6e68b",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff045ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "pca2 = PCA()\n",
    "pca2.fit(Xtrn_mn)\n",
    "K = [5,20,50,200,400]\n",
    "\n",
    "first_instance_index = []\n",
    "for i in range(10):\n",
    "    index = np.argmax(Ytrn == i)\n",
    "    first_instance_index.append(index)\n",
    "\n",
    "reconstructed_images = {}\n",
    "for i,index in enumerate(first_instance_index):\n",
    "    reconstructed_images[i]=[]\n",
    "    for j, k in enumerate(K):\n",
    "        pcak = PCA(n_components = k)\n",
    "        pcak.components_ = pca2.components_[:k]\n",
    "        pcak.mean_ = pca2.mean_\n",
    "        reduced_instance = pcak.transform(Xtrn_mn[index].reshape(1, -1))\n",
    "        reconstructed_image = pcak.inverse_transform(reduced_instance)\n",
    "        reconstructed_images[i].append(reconstructed_image)\n",
    "        \n",
    "plt.figure(figsize=(16, 20))\n",
    "for i in range(10):\n",
    "    for j, image in enumerate(reconstructed_images[i]):\n",
    "        plt.subplot(10, 6, i * 6 + j+1)\n",
    "        plt.imshow((image+Xmean).reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "        if i == 0:\n",
    "            plt.title('K = {}'.format(K[j]))\n",
    "        plt.axis('off')\n",
    "        if j ==4:\n",
    "            index = first_instance_index[i]\n",
    "            plt.subplot(10, 6, i * 6 + j+2)\n",
    "            plt.imshow((Xtrn_mn[index]+Xmean).reshape(28, 28), cmap=plt.cm.gray_r)\n",
    "            if i == 0:\n",
    "                plt.title('Original')\n",
    "            plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25cbaa",
   "metadata": {},
   "source": [
    "#(2) ***Your text goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22d15d",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c8610",
   "metadata": {},
   "source": [
    "# ========== Question 2.5 --- [6 marks] ==========\n",
    "\n",
    "We now would like to know how the training data **Xtrn_mn** distribute in a vector space. To visualise distributions, we reduce the dimensionality of the data to two dimensions using PCA and plot the dimensionality-reduced data on the two-dimensional plane spanned by the first principal components. Note that each instance in the data set is now displayed as a single point on the plane.\n",
    "1. [Code] Plot all the training instances (**Xtrn_mn**) on the two-dimensional PCA plane, where each instance is displayed as a small point with a colour specific to the class of the instance. Use the tab10 colormap for plotting (i.e. cmap=\"tabl10\"), and adjust the marker size so that points do not overlap each other very much.\n",
    "2. [Text] Give comments on the separation of the classes, and explain your findings briefly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e89796e",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f40646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "pca_2d = PCA(n_components=2)\n",
    "Xtrn_2d = pca_2d.fit_transform(Xtrn_mn)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(Xtrn_2d[:, 0], Xtrn_2d[:, 1], c=Ytrn, cmap='tab10', alpha=.5, lw=2)\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.title(\"2 dimensioanal PCA Plane of all instances\")\n",
    "plt.colorbar(label='Class Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b277bc",
   "metadata": {},
   "source": [
    "#(2) ***Your text goes here***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246e180",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8290a",
   "metadata": {},
   "source": [
    "# ========== Question 2.6 --- [8 marks] ==========\n",
    "\n",
    "We consider applying multiclass classification to the data set. Make sure that you use **Xtrn_mn** for training and **Xtst_mn** for testing. \n",
    "1. [Code] Carry out a classification experiment using sklearn's **LogisticRegression** with \"random_state=0\", and report the classification accuracy and confusion matrix for the training set and test set respectively. Use sklearn's **ConfusionMatrixDisplay** to display the confusion matrix. Note that you may ignore a warning message in the training.\n",
    "2. [Code] Run a classification experiment with SVM and report the classification accuracy and confusion matrix for the training set and test set respectively. Use sklearn's **SVC** with \"random_state=0\".\n",
    "3. [Text] Based on the results obtained in 1 and 2, explain your findings and give brief discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88cb57a",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f14eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Your code goes here\n",
    "LG = models[1][1]\n",
    "LG.fit(Xtrn_mn, Ytrn)\n",
    "    \n",
    "Xtrn_pred = LG.predict(Xtrn_mn)\n",
    "Xtst_pred = LG.predict(Xtst_mn)\n",
    "Xtrn_cm = confusion_matrix(Ytrn, Xtrn_pred)\n",
    "Xtst_cm = confusion_matrix(Ytst, Xtst_pred)\n",
    "\n",
    "print('Accuracy of training set: {:.3f}'.format(accuracy_score(Ytrn, Xtrn_pred)))\n",
    "\n",
    "Xtrn_cm_norm = Xtrn_cm / Xtrn_cm.sum(axis=1)[:, np.newaxis]\n",
    "display_train = ConfusionMatrixDisplay(Xtrn_cm_norm, display_labels=classes)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "display_train.plot(ax = ax)\n",
    "plt.title('Confusion matrix for train set')\n",
    "plt.show()\n",
    "\n",
    "print('Test set:')\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(Ytst,test_pred)))\n",
    "\n",
    "Xtst_cm_norm = Xtst_cm / Xtst_cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "display_test = ConfusionMatrixDisplay(cm_norm_test, display_labels=classes)\n",
    "display_test.plot(ax = ax)\n",
    "plt.title('Confusion matrix for test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f42f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code goes here\n",
    "svc = SVC(random_state=0)\n",
    "svc.fit(Xtrn_mn,Ytrn)\n",
    "\n",
    "Xtrn_pred = svc.predict(Xtrn_mn)\n",
    "Xtst_pred = svc.predict(Xtst_mn)\n",
    "Xtrn_cm = confusion_matrix(Ytrn, Xtrn_pred)\n",
    "Xtst_cm = confusion_matrix(Ytst, Xtst_pred)\n",
    "\n",
    "print('Accuracy of training set: {:.3f}'.format(accuracy_score(Ytrn, Xtrn_pred)))\n",
    "\n",
    "Xtrn_cm_norm = Xtrn_cm / Xtrn_cm.sum(axis=1)[:, np.newaxis]\n",
    "display_train = ConfusionMatrixDisplay(Xtrn_cm_norm, display_labels=classes)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "display_train.plot(ax = ax)\n",
    "plt.title('Confusion matrix for train set')\n",
    "plt.show()\n",
    "\n",
    "print('Test set:')\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(Ytst,test_pred)))\n",
    "\n",
    "Xtst_cm_norm = Xtst_cm / Xtst_cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "display_test = ConfusionMatrixDisplay(cm_norm_test, display_labels=classes)\n",
    "display_test.plot(ax = ax)\n",
    "plt.title('Confusion matrix for test set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d33422",
   "metadata": {},
   "source": [
    "#(3) Your text goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce28085",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f29734",
   "metadata": {},
   "source": [
    "# ========== Question 2.7 --- [18 marks] ==========\n",
    "\n",
    "This is a mini project, in which you are asked to improve the classification accuracy for the logistic regression model as much as possible from the one obtained in Question 2.6. \n",
    "1. [Text] Discuss possible approaches, and decide the one(s) you implement. Note that you should stick to the multinomial logistic regression model and should not use other classification models.\n",
    "2. [Code and Text] Implement the approach you have chosen, carry out a classification experiment and report accuracy for the training set and test set respectively. Note that training and parameter tuning should be done on the training set and not on the test set. In case that you run parameter tuning, show and explain the result clearly.\n",
    "3. [Text] Making a quick investigation to the result, report your findings and give brief discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27029f59",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "## Your answers for Question 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316090a4",
   "metadata": {},
   "source": [
    "#(1) Your text goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) Your code and text goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c3e13",
   "metadata": {},
   "source": [
    "#(3) Your text goes here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
